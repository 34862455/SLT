2025-06-05 10:23:53,875 Hello! This is Joey-NMT.
2025-06-05 10:23:53,878 Total params: 22679581
2025-06-05 10:23:53,879 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.lut.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']
2025-06-05 10:23:53,897 cfg.name                           : sign_experiment
2025-06-05 10:23:53,898 cfg.data.data_path                 : /home/minneke/Documents/Projects/SignExperiments/data
2025-06-05 10:23:53,898 cfg.data.version                   : phoenix_2014_trans
2025-06-05 10:23:53,898 cfg.data.sgn                       : sign
2025-06-05 10:23:53,898 cfg.data.txt                       : text
2025-06-05 10:23:53,898 cfg.data.gls                       : gloss
2025-06-05 10:23:53,898 cfg.data.train                     : DSG_train_subset.pt
2025-06-05 10:23:53,898 cfg.data.dev                       : DSG_dev_subset.pt
2025-06-05 10:23:53,898 cfg.data.test                      : DSG_test_subset.pt
2025-06-05 10:23:53,898 cfg.data.feature_size              : 1024
2025-06-05 10:23:53,898 cfg.data.level                     : word
2025-06-05 10:23:53,898 cfg.data.txt_lowercase             : True
2025-06-05 10:23:53,898 cfg.data.max_sent_length           : 400
2025-06-05 10:23:53,898 cfg.data.random_train_subset       : -1
2025-06-05 10:23:53,898 cfg.data.random_dev_subset         : -1
2025-06-05 10:23:53,898 cfg.data.include_masks             : False
2025-06-05 10:23:53,898 cfg.testing.recognition_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2025-06-05 10:23:53,898 cfg.testing.translation_beam_sizes : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
2025-06-05 10:23:53,898 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]
2025-06-05 10:23:53,898 cfg.training.reset_best_ckpt       : False
2025-06-05 10:23:53,898 cfg.training.reset_scheduler       : False
2025-06-05 10:23:53,898 cfg.training.reset_optimizer       : False
2025-06-05 10:23:53,898 cfg.training.random_seed           : 42
2025-06-05 10:23:53,898 cfg.training.model_dir             : ./sign_sample_model
2025-06-05 10:23:53,898 cfg.training.recognition_loss_weight : 1.0
2025-06-05 10:23:53,898 cfg.training.translation_loss_weight : 1.0
2025-06-05 10:23:53,898 cfg.training.eval_metric           : bleu
2025-06-05 10:23:53,898 cfg.training.optimizer             : adam
2025-06-05 10:23:53,898 cfg.training.learning_rate         : 0.001
2025-06-05 10:23:53,898 cfg.training.batch_size            : 32
2025-06-05 10:23:53,898 cfg.training.num_valid_log         : 5
2025-06-05 10:23:53,898 cfg.training.epochs                : 5000000
2025-06-05 10:23:53,898 cfg.training.early_stopping_metric : eval_metric
2025-06-05 10:23:53,898 cfg.training.batch_type            : sentence
2025-06-05 10:23:53,898 cfg.training.translation_normalization : batch
2025-06-05 10:23:53,898 cfg.training.eval_recognition_beam_size : 1
2025-06-05 10:23:53,898 cfg.training.eval_translation_beam_size : 1
2025-06-05 10:23:53,898 cfg.training.eval_translation_beam_alpha : -1
2025-06-05 10:23:53,898 cfg.training.overwrite             : True
2025-06-05 10:23:53,898 cfg.training.shuffle               : True
2025-06-05 10:23:53,898 cfg.training.use_cuda              : True
2025-06-05 10:23:53,898 cfg.training.translation_max_output_length : 30
2025-06-05 10:23:53,899 cfg.training.keep_last_ckpts       : 1
2025-06-05 10:23:53,899 cfg.training.batch_multiplier      : 1
2025-06-05 10:23:53,899 cfg.training.logging_freq          : 100
2025-06-05 10:23:53,899 cfg.training.validation_freq       : 100
2025-06-05 10:23:53,899 cfg.training.betas                 : [0.9, 0.998]
2025-06-05 10:23:53,899 cfg.training.scheduling            : plateau
2025-06-05 10:23:53,899 cfg.training.learning_rate_min     : 1e-06
2025-06-05 10:23:53,899 cfg.training.weight_decay          : 0.001
2025-06-05 10:23:53,899 cfg.training.patience              : 8
2025-06-05 10:23:53,899 cfg.training.decrease_factor       : 0.7
2025-06-05 10:23:53,899 cfg.training.label_smoothing       : 0.0
2025-06-05 10:23:53,899 cfg.model.initializer              : xavier
2025-06-05 10:23:53,899 cfg.model.bias_initializer         : zeros
2025-06-05 10:23:53,899 cfg.model.init_gain                : 1.0
2025-06-05 10:23:53,899 cfg.model.embed_initializer        : xavier
2025-06-05 10:23:53,899 cfg.model.embed_init_gain          : 1.0
2025-06-05 10:23:53,899 cfg.model.tied_softmax             : False
2025-06-05 10:23:53,899 cfg.model.encoder.type             : transformer
2025-06-05 10:23:53,899 cfg.model.encoder.num_layers       : 3
2025-06-05 10:23:53,899 cfg.model.encoder.num_heads        : 8
2025-06-05 10:23:53,899 cfg.model.encoder.embeddings.embedding_dim : 512
2025-06-05 10:23:53,899 cfg.model.encoder.embeddings.scale : False
2025-06-05 10:23:53,899 cfg.model.encoder.embeddings.dropout : 0.1
2025-06-05 10:23:53,899 cfg.model.encoder.embeddings.norm_type : batch
2025-06-05 10:23:53,899 cfg.model.encoder.embeddings.activation_type : softsign
2025-06-05 10:23:53,899 cfg.model.encoder.hidden_size      : 512
2025-06-05 10:23:53,900 cfg.model.encoder.ff_size          : 2048
2025-06-05 10:23:53,900 cfg.model.encoder.dropout          : 0.1
2025-06-05 10:23:53,900 cfg.model.decoder.type             : transformer
2025-06-05 10:23:53,900 cfg.model.decoder.num_layers       : 3
2025-06-05 10:23:53,900 cfg.model.decoder.num_heads        : 8
2025-06-05 10:23:53,900 cfg.model.decoder.embeddings.embedding_dim : 512
2025-06-05 10:23:53,900 cfg.model.decoder.embeddings.scale : False
2025-06-05 10:23:53,900 cfg.model.decoder.embeddings.dropout : 0.1
2025-06-05 10:23:53,900 cfg.model.decoder.embeddings.norm_type : batch
2025-06-05 10:23:53,900 cfg.model.decoder.embeddings.activation_type : softsign
2025-06-05 10:23:53,900 cfg.model.decoder.hidden_size      : 512
2025-06-05 10:23:53,900 cfg.model.decoder.ff_size          : 2048
2025-06-05 10:23:53,900 cfg.model.decoder.dropout          : 0.1
2025-06-05 10:23:53,900 Data set sizes: 
	train 5,
	valid 5,
	test 5
2025-06-05 10:23:53,900 First training example:
	[GLS] JETZT WETTER MORGEN DONNERSTAG ZWOELF FEBRUAR
	[TXT] und nun die wettervorhersage für morgen donnerstag den zwölften august
2025-06-05 10:23:53,900 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) REGEN (4) KOENNEN (5) GEWITTER (6) NORDWEST (7) ORT (8) BLEIBEN (9) DAZU
2025-06-05 10:23:53,900 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) und (5) auch (6) es (7) im (8) mit (9) nordwesten
2025-06-05 10:23:53,900 Number of unique glosses (types): 29
2025-06-05 10:23:53,900 Number of unique words (types): 65
2025-06-05 10:23:53,900 SignModel(
	encoder=TransformerEncoder(num_layers=3, num_heads=8),
	decoder=TransformerDecoder(num_layers=3, num_heads=8),
	sgn_embed=SpatialEmbeddings(embedding_dim=512, input_size=1024),
	txt_embed=Embeddings(embedding_dim=512, vocab_size=65))
2025-06-05 10:23:53,900 EPOCH 1
2025-06-05 10:23:53,988 Epoch   1: Total Training Recognition Loss 5.28  Total Training Translation Loss 74.77 
2025-06-05 10:23:53,988 EPOCH 2
2025-06-05 10:23:54,032 Epoch   2: Total Training Recognition Loss 3.87  Total Training Translation Loss 86.83 
2025-06-05 10:23:54,033 EPOCH 3
2025-06-05 10:23:54,093 Epoch   3: Total Training Recognition Loss 4.31  Total Training Translation Loss 86.85 
2025-06-05 10:23:54,094 EPOCH 4
2025-06-05 10:23:54,189 Epoch   4: Total Training Recognition Loss 3.47  Total Training Translation Loss 82.32 
2025-06-05 10:23:54,190 EPOCH 5
2025-06-05 10:23:54,228 Epoch   5: Total Training Recognition Loss 3.22  Total Training Translation Loss 79.18 
2025-06-05 10:23:54,229 EPOCH 6
2025-06-05 10:23:54,267 Epoch   6: Total Training Recognition Loss 3.16  Total Training Translation Loss 76.17 
2025-06-05 10:23:54,268 EPOCH 7
2025-06-05 10:23:54,306 Epoch   7: Total Training Recognition Loss 2.91  Total Training Translation Loss 73.13 
2025-06-05 10:23:54,307 EPOCH 8
2025-06-05 10:23:54,346 Epoch   8: Total Training Recognition Loss 2.93  Total Training Translation Loss 68.08 
2025-06-05 10:23:54,346 EPOCH 9
2025-06-05 10:23:54,384 Epoch   9: Total Training Recognition Loss 2.82  Total Training Translation Loss 60.49 
2025-06-05 10:23:54,384 EPOCH 10
2025-06-05 10:23:54,422 Epoch  10: Total Training Recognition Loss 2.91  Total Training Translation Loss 44.96 
2025-06-05 10:23:54,422 EPOCH 11
2025-06-05 10:23:54,459 Epoch  11: Total Training Recognition Loss 3.27  Total Training Translation Loss 108.62 
2025-06-05 10:23:54,460 EPOCH 12
2025-06-05 10:23:54,496 Epoch  12: Total Training Recognition Loss 2.61  Total Training Translation Loss 43.00 
2025-06-05 10:23:54,497 EPOCH 13
2025-06-05 10:23:54,533 Epoch  13: Total Training Recognition Loss 2.71  Total Training Translation Loss 46.36 
2025-06-05 10:23:54,533 EPOCH 14
2025-06-05 10:23:54,570 Epoch  14: Total Training Recognition Loss 2.51  Total Training Translation Loss 47.09 
2025-06-05 10:23:54,570 EPOCH 15
2025-06-05 10:23:54,606 Epoch  15: Total Training Recognition Loss 2.33  Total Training Translation Loss 44.81 
2025-06-05 10:23:54,606 EPOCH 16
2025-06-05 10:23:54,643 Epoch  16: Total Training Recognition Loss 2.30  Total Training Translation Loss 41.56 
2025-06-05 10:23:54,643 EPOCH 17
2025-06-05 10:23:54,679 Epoch  17: Total Training Recognition Loss 2.33  Total Training Translation Loss 37.02 
2025-06-05 10:23:54,679 EPOCH 18
2025-06-05 10:23:54,715 Epoch  18: Total Training Recognition Loss 2.28  Total Training Translation Loss 34.77 
2025-06-05 10:23:54,715 EPOCH 19
2025-06-05 10:23:54,750 Epoch  19: Total Training Recognition Loss 2.22  Total Training Translation Loss 30.28 
2025-06-05 10:23:54,750 EPOCH 20
2025-06-05 10:23:54,786 Epoch  20: Total Training Recognition Loss 2.17  Total Training Translation Loss 26.76 
2025-06-05 10:23:54,786 EPOCH 21
2025-06-05 10:23:54,822 Epoch  21: Total Training Recognition Loss 2.13  Total Training Translation Loss 22.22 
2025-06-05 10:23:54,822 EPOCH 22
2025-06-05 10:23:54,858 Epoch  22: Total Training Recognition Loss 2.06  Total Training Translation Loss 19.28 
2025-06-05 10:23:54,858 EPOCH 23
2025-06-05 10:23:54,894 Epoch  23: Total Training Recognition Loss 2.06  Total Training Translation Loss 17.97 
2025-06-05 10:23:54,894 EPOCH 24
2025-06-05 10:23:54,933 Epoch  24: Total Training Recognition Loss 1.99  Total Training Translation Loss 15.25 
2025-06-05 10:23:54,934 EPOCH 25
2025-06-05 10:23:54,972 Epoch  25: Total Training Recognition Loss 1.98  Total Training Translation Loss 13.78 
2025-06-05 10:23:54,972 EPOCH 26
2025-06-05 10:23:55,010 Epoch  26: Total Training Recognition Loss 1.97  Total Training Translation Loss 12.47 
2025-06-05 10:23:55,010 EPOCH 27
2025-06-05 10:23:55,047 Epoch  27: Total Training Recognition Loss 1.91  Total Training Translation Loss 10.33 
2025-06-05 10:23:55,047 EPOCH 28
2025-06-05 10:23:55,084 Epoch  28: Total Training Recognition Loss 1.88  Total Training Translation Loss 8.93 
2025-06-05 10:23:55,084 EPOCH 29
2025-06-05 10:23:55,123 Epoch  29: Total Training Recognition Loss 1.90  Total Training Translation Loss 7.79 
2025-06-05 10:23:55,124 EPOCH 30
2025-06-05 10:23:55,162 Epoch  30: Total Training Recognition Loss 1.80  Total Training Translation Loss 6.49 
2025-06-05 10:23:55,162 EPOCH 31
2025-06-05 10:23:55,198 Epoch  31: Total Training Recognition Loss 1.79  Total Training Translation Loss 5.79 
2025-06-05 10:23:55,199 EPOCH 32
2025-06-05 10:23:55,235 Epoch  32: Total Training Recognition Loss 1.78  Total Training Translation Loss 4.45 
2025-06-05 10:23:55,235 EPOCH 33
2025-06-05 10:23:55,273 Epoch  33: Total Training Recognition Loss 1.80  Total Training Translation Loss 4.07 
2025-06-05 10:23:55,273 EPOCH 34
2025-06-05 10:23:55,311 Epoch  34: Total Training Recognition Loss 1.77  Total Training Translation Loss 3.26 
2025-06-05 10:23:55,311 EPOCH 35
2025-06-05 10:23:55,382 Epoch  35: Total Training Recognition Loss 1.72  Total Training Translation Loss 2.68 
2025-06-05 10:23:55,383 EPOCH 36
2025-06-05 10:23:55,447 Epoch  36: Total Training Recognition Loss 1.73  Total Training Translation Loss 2.31 
2025-06-05 10:23:55,447 EPOCH 37
2025-06-05 10:23:55,489 Epoch  37: Total Training Recognition Loss 1.76  Total Training Translation Loss 1.78 
2025-06-05 10:23:55,489 EPOCH 38
2025-06-05 10:23:55,525 Epoch  38: Total Training Recognition Loss 1.70  Total Training Translation Loss 1.57 
2025-06-05 10:23:55,525 EPOCH 39
2025-06-05 10:23:55,561 Epoch  39: Total Training Recognition Loss 1.67  Total Training Translation Loss 1.73 
2025-06-05 10:23:55,561 EPOCH 40
2025-06-05 10:23:55,597 Epoch  40: Total Training Recognition Loss 1.65  Total Training Translation Loss 1.64 
2025-06-05 10:23:55,597 EPOCH 41
2025-06-05 10:23:55,635 Epoch  41: Total Training Recognition Loss 1.66  Total Training Translation Loss 0.95 
2025-06-05 10:23:55,636 EPOCH 42
2025-06-05 10:23:55,673 Epoch  42: Total Training Recognition Loss 1.65  Total Training Translation Loss 0.90 
2025-06-05 10:23:55,674 EPOCH 43
2025-06-05 10:23:55,712 Epoch  43: Total Training Recognition Loss 1.69  Total Training Translation Loss 1.41 
2025-06-05 10:23:55,712 EPOCH 44
2025-06-05 10:23:55,751 Epoch  44: Total Training Recognition Loss 1.63  Total Training Translation Loss 0.96 
2025-06-05 10:23:55,751 EPOCH 45
2025-06-05 10:23:55,786 Epoch  45: Total Training Recognition Loss 1.63  Total Training Translation Loss 0.94 
2025-06-05 10:23:55,786 EPOCH 46
2025-06-05 10:23:55,822 Epoch  46: Total Training Recognition Loss 1.57  Total Training Translation Loss 0.79 
2025-06-05 10:23:55,822 EPOCH 47
2025-06-05 10:23:55,858 Epoch  47: Total Training Recognition Loss 1.62  Total Training Translation Loss 0.59 
2025-06-05 10:23:55,858 EPOCH 48
2025-06-05 10:23:55,896 Epoch  48: Total Training Recognition Loss 1.57  Total Training Translation Loss 0.57 
2025-06-05 10:23:55,896 EPOCH 49
2025-06-05 10:23:55,932 Epoch  49: Total Training Recognition Loss 1.58  Total Training Translation Loss 0.75 
2025-06-05 10:23:55,933 EPOCH 50
2025-06-05 10:23:55,972 Epoch  50: Total Training Recognition Loss 1.54  Total Training Translation Loss 0.38 
2025-06-05 10:23:55,972 EPOCH 51
2025-06-05 10:23:56,008 Epoch  51: Total Training Recognition Loss 1.52  Total Training Translation Loss 0.48 
2025-06-05 10:23:56,008 EPOCH 52
2025-06-05 10:23:56,043 Epoch  52: Total Training Recognition Loss 1.51  Total Training Translation Loss 0.38 
2025-06-05 10:23:56,043 EPOCH 53
2025-06-05 10:23:56,080 Epoch  53: Total Training Recognition Loss 1.45  Total Training Translation Loss 0.41 
2025-06-05 10:23:56,080 EPOCH 54
2025-06-05 10:23:56,117 Epoch  54: Total Training Recognition Loss 1.50  Total Training Translation Loss 0.28 
2025-06-05 10:23:56,117 EPOCH 55
2025-06-05 10:23:56,154 Epoch  55: Total Training Recognition Loss 1.47  Total Training Translation Loss 0.20 
2025-06-05 10:23:56,154 EPOCH 56
2025-06-05 10:23:56,190 Epoch  56: Total Training Recognition Loss 1.47  Total Training Translation Loss 0.21 
2025-06-05 10:23:56,190 EPOCH 57
2025-06-05 10:23:56,236 Epoch  57: Total Training Recognition Loss 1.41  Total Training Translation Loss 0.38 
2025-06-05 10:23:56,236 EPOCH 58
2025-06-05 10:23:56,307 Epoch  58: Total Training Recognition Loss 1.45  Total Training Translation Loss 0.20 
2025-06-05 10:23:56,307 EPOCH 59
2025-06-05 10:23:56,399 Epoch  59: Total Training Recognition Loss 1.43  Total Training Translation Loss 0.25 
2025-06-05 10:23:56,400 EPOCH 60
2025-06-05 10:23:56,471 Epoch  60: Total Training Recognition Loss 1.38  Total Training Translation Loss 0.42 
2025-06-05 10:23:56,471 EPOCH 61
2025-06-05 10:23:56,507 Epoch  61: Total Training Recognition Loss 1.40  Total Training Translation Loss 0.19 
2025-06-05 10:23:56,508 EPOCH 62
2025-06-05 10:23:56,548 Epoch  62: Total Training Recognition Loss 1.39  Total Training Translation Loss 0.16 
2025-06-05 10:23:56,548 EPOCH 63
2025-06-05 10:23:56,585 Epoch  63: Total Training Recognition Loss 1.37  Total Training Translation Loss 0.11 
2025-06-05 10:23:56,586 EPOCH 64
2025-06-05 10:23:56,621 Epoch  64: Total Training Recognition Loss 1.37  Total Training Translation Loss 0.82 
2025-06-05 10:23:56,622 EPOCH 65
2025-06-05 10:23:56,659 Epoch  65: Total Training Recognition Loss 1.38  Total Training Translation Loss 0.10 
2025-06-05 10:23:56,660 EPOCH 66
2025-06-05 10:23:56,698 Epoch  66: Total Training Recognition Loss 1.32  Total Training Translation Loss 0.11 
2025-06-05 10:23:56,698 EPOCH 67
2025-06-05 10:23:56,737 Epoch  67: Total Training Recognition Loss 1.30  Total Training Translation Loss 0.11 
2025-06-05 10:23:56,737 EPOCH 68
2025-06-05 10:23:56,774 Epoch  68: Total Training Recognition Loss 1.27  Total Training Translation Loss 0.05 
2025-06-05 10:23:56,775 EPOCH 69
2025-06-05 10:23:56,811 Epoch  69: Total Training Recognition Loss 1.27  Total Training Translation Loss 0.12 
2025-06-05 10:23:56,811 EPOCH 70
2025-06-05 10:23:56,850 Epoch  70: Total Training Recognition Loss 1.28  Total Training Translation Loss 0.32 
2025-06-05 10:23:56,850 EPOCH 71
2025-06-05 10:23:56,886 Epoch  71: Total Training Recognition Loss 1.28  Total Training Translation Loss 0.13 
2025-06-05 10:23:56,886 EPOCH 72
2025-06-05 10:23:56,922 Epoch  72: Total Training Recognition Loss 1.25  Total Training Translation Loss 0.36 
2025-06-05 10:23:56,922 EPOCH 73
2025-06-05 10:23:56,958 Epoch  73: Total Training Recognition Loss 1.24  Total Training Translation Loss 0.07 
2025-06-05 10:23:56,959 EPOCH 74
2025-06-05 10:23:56,997 Epoch  74: Total Training Recognition Loss 1.21  Total Training Translation Loss 0.11 
2025-06-05 10:23:56,997 EPOCH 75
2025-06-05 10:23:57,032 Epoch  75: Total Training Recognition Loss 1.20  Total Training Translation Loss 1.02 
2025-06-05 10:23:57,032 EPOCH 76
2025-06-05 10:23:57,070 Epoch  76: Total Training Recognition Loss 1.18  Total Training Translation Loss 0.07 
2025-06-05 10:23:57,070 EPOCH 77
2025-06-05 10:23:57,108 Epoch  77: Total Training Recognition Loss 1.13  Total Training Translation Loss 0.12 
2025-06-05 10:23:57,109 EPOCH 78
2025-06-05 10:23:57,150 Epoch  78: Total Training Recognition Loss 1.12  Total Training Translation Loss 0.17 
2025-06-05 10:23:57,151 EPOCH 79
2025-06-05 10:23:57,240 Epoch  79: Total Training Recognition Loss 1.13  Total Training Translation Loss 0.29 
2025-06-05 10:23:57,240 EPOCH 80
2025-06-05 10:23:57,288 Epoch  80: Total Training Recognition Loss 1.12  Total Training Translation Loss 0.08 
2025-06-05 10:23:57,289 EPOCH 81
2025-06-05 10:23:57,324 Epoch  81: Total Training Recognition Loss 1.10  Total Training Translation Loss 0.17 
2025-06-05 10:23:57,324 EPOCH 82
2025-06-05 10:23:57,360 Epoch  82: Total Training Recognition Loss 1.09  Total Training Translation Loss 0.18 
2025-06-05 10:23:57,360 EPOCH 83
2025-06-05 10:23:57,396 Epoch  83: Total Training Recognition Loss 1.05  Total Training Translation Loss 0.06 
2025-06-05 10:23:57,396 EPOCH 84
2025-06-05 10:23:57,431 Epoch  84: Total Training Recognition Loss 1.06  Total Training Translation Loss 0.06 
2025-06-05 10:23:57,431 EPOCH 85
2025-06-05 10:23:57,470 Epoch  85: Total Training Recognition Loss 1.05  Total Training Translation Loss 0.09 
2025-06-05 10:23:57,470 EPOCH 86
2025-06-05 10:23:57,508 Epoch  86: Total Training Recognition Loss 1.00  Total Training Translation Loss 0.29 
2025-06-05 10:23:57,508 EPOCH 87
2025-06-05 10:23:57,546 Epoch  87: Total Training Recognition Loss 1.00  Total Training Translation Loss 0.08 
2025-06-05 10:23:57,546 EPOCH 88
2025-06-05 10:23:57,584 Epoch  88: Total Training Recognition Loss 1.01  Total Training Translation Loss 0.29 
2025-06-05 10:23:57,585 EPOCH 89
2025-06-05 10:23:57,623 Epoch  89: Total Training Recognition Loss 0.94  Total Training Translation Loss 0.10 
2025-06-05 10:23:57,623 EPOCH 90
2025-06-05 10:23:57,661 Epoch  90: Total Training Recognition Loss 0.96  Total Training Translation Loss 0.27 
2025-06-05 10:23:57,662 EPOCH 91
2025-06-05 10:23:57,699 Epoch  91: Total Training Recognition Loss 0.94  Total Training Translation Loss 0.06 
2025-06-05 10:23:57,699 EPOCH 92
2025-06-05 10:23:57,738 Epoch  92: Total Training Recognition Loss 0.94  Total Training Translation Loss 0.04 
2025-06-05 10:23:57,738 EPOCH 93
2025-06-05 10:23:57,776 Epoch  93: Total Training Recognition Loss 0.90  Total Training Translation Loss 0.05 
2025-06-05 10:23:57,777 EPOCH 94
2025-06-05 10:23:57,815 Epoch  94: Total Training Recognition Loss 0.86  Total Training Translation Loss 0.06 
2025-06-05 10:23:57,815 EPOCH 95
2025-06-05 10:23:57,851 Epoch  95: Total Training Recognition Loss 0.86  Total Training Translation Loss 0.07 
2025-06-05 10:23:57,851 EPOCH 96
2025-06-05 10:23:57,889 Epoch  96: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.04 
2025-06-05 10:23:57,889 EPOCH 97
2025-06-05 10:23:57,927 Epoch  97: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.04 
2025-06-05 10:23:57,927 EPOCH 98
2025-06-05 10:23:57,965 Epoch  98: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.22 
2025-06-05 10:23:57,965 EPOCH 99
2025-06-05 10:23:58,003 Epoch  99: Total Training Recognition Loss 0.81  Total Training Translation Loss 0.02 
2025-06-05 10:23:58,003 EPOCH 100
2025-06-05 10:23:58,041 [Epoch: 100 Step: 00000100] Batch Recognition Loss:   0.737829 => Gls Tokens per Sec:     1003 || Batch Translation Loss:   0.059326 => Txt Tokens per Sec:     2061 || Lr: 0.001000
2025-06-05 10:23:58,187 Hooray! New best validation result [eval_metric]!
2025-06-05 10:23:58,188 Saving new checkpoint.
2025-06-05 10:23:58,445 Validation result at epoch 100, step      100: duration: 0.4032s
	Recognition Beam Size: 1	Translation Beam Size: 1	Translation Beam Alpha: -1
	Recognition Loss: 5.76165	Translation Loss: 690.54437	PPL: 22200.93750
	Eval Metric: BLEU
	WER 85.71	(DEL: 65.71,	INS: 0.00,	SUB: 20.00)
	BLEU-4 8.19	(BLEU-1: 16.90,	BLEU-2: 11.32,	BLEU-3: 9.43,	BLEU-4: 8.19)
	CHRF 32.49	ROUGE 20.22
2025-06-05 10:23:58,448 Logging Recognition and Translation Outputs
2025-06-05 10:23:58,448 ========================================================================================================================
2025-06-05 10:23:58,448 Logging Sequence: 11August_2010_Wednesday_tagesschau-2
2025-06-05 10:23:58,449 	Gloss Reference :	DRUCK TIEF     KOMMEN
2025-06-05 10:23:58,449 	Gloss Hypothesis:	WOLKE SPEZIELL LOCH  
2025-06-05 10:23:58,449 	Gloss Alignment :	S     S        S     
2025-06-05 10:23:58,449 	--------------------------------------------------------------------------------------------------------------------
2025-06-05 10:23:58,449 	Text Reference  :	tiefer luftdruck bestimmt     in     den  nächsten tagen unser wetter    
2025-06-05 10:23:58,449 	Text Hypothesis :	****** größere   wolkenlücken finden sich vor      allem im    nordwesten
2025-06-05 10:23:58,449 	Text Alignment  :	D      S         S            S      S    S        S     S     S         
2025-06-05 10:23:58,449 ========================================================================================================================
2025-06-05 10:23:58,449 Logging Sequence: 11August_2010_Wednesday_tagesschau-3
2025-06-05 10:23:58,449 	Gloss Reference :	ES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN
2025-06-05 10:23:58,449 	Gloss Hypothesis:	*********** **** ***** *** ******* REGEN ******** *******
2025-06-05 10:23:58,449 	Gloss Alignment :	D           D    D     D   D             D        D      
2025-06-05 10:23:58,449 	--------------------------------------------------------------------------------------------------------------------
2025-06-05 10:23:58,451 	Text Reference  :	** ********** ****** ** ***** ***** ***** ******* das   bedeutet viele wolken    und       immer    wieder    zum    teil    kräftige schauer und gewitter
2025-06-05 10:23:58,451 	Text Hypothesis :	im nordwesten bleibt es heute nacht meist trocken sonst muss     mit   teilweise kräftigen schauern gerechnet werden örtlich mit      blitz   und donner  
2025-06-05 10:23:58,451 	Text Alignment  :	I  I          I      I  I     I     I     I       S     S        S     S         S         S        S         S      S       S        S           S       
2025-06-05 10:23:58,451 ========================================================================================================================
2025-06-05 10:23:58,451 Logging Sequence: 11August_2010_Wednesday_tagesschau-8
2025-06-05 10:23:58,451 	Gloss Reference :	WIND MAESSIG SCHWACH REGION WENN GEWITTER WIND  KOENNEN
2025-06-05 10:23:58,451 	Gloss Hypothesis:	**** ******* ******* ****** **** KOENNEN  REGEN KOENNEN
2025-06-05 10:23:58,451 	Gloss Alignment :	D    D       D       D      D    S        S            
2025-06-05 10:23:58,451 	--------------------------------------------------------------------------------------------------------------------
2025-06-05 10:23:58,452 	Text Reference  :	meist weht nur ein schwacher wind        aus    unterschiedlichen richtungen der    bei schauern und  gewittern stark            böig sein   kann   
2025-06-05 10:23:58,452 	Text Hypothesis :	***** **** *** *** ********* mancherorts regnet es                auch       länger und ergiebig auch lokale    überschwemmungen sind wieder möglich
2025-06-05 10:23:58,452 	Text Alignment  :	D     D    D   D   D         S           S      S                 S          S      S   S        S    S         S                S    S      S      
2025-06-05 10:23:58,452 ========================================================================================================================
2025-06-05 10:23:58,452 Logging Sequence: 25October_2010_Monday_tagesschau-22
2025-06-05 10:23:58,452 	Gloss Reference :	MITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND
2025-06-05 10:23:58,453 	Gloss Hypothesis:	******** REGEN ******* ******** ************** **** ***** ****
2025-06-05 10:23:58,453 	Gloss Alignment :	D              D       D        D              D    D     D   
2025-06-05 10:23:58,453 	--------------------------------------------------------------------------------------------------------------------
2025-06-05 10:23:58,454 	Text Reference  :	**** am *** **** ** ********** mittwoch hier und      da      nieselregen in       der nordwesthälfte an      den      küsten kräftiger wind       
2025-06-05 10:23:58,454 	Text Hypothesis :	auch am tag gibt es verbreitet zum      teil kräftige schauer oder        gewitter und in             manchen regionen fallen ergiebige regenmengen
2025-06-05 10:23:58,454 	Text Alignment  :	I       I   I    I  I          S        S    S        S       S           S        S   S              S       S        S      S         S          
2025-06-05 10:23:58,454 ========================================================================================================================
2025-06-05 10:23:58,454 Logging Sequence: 05May_2011_Thursday_tagesschau-25
2025-06-05 10:23:58,454 	Gloss Reference :	JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM
2025-06-05 10:23:58,454 	Gloss Hypothesis:	JETZT WETTER ************ ****** ******* ******* DONNERSTAG FEBRUAR          
2025-06-05 10:23:58,454 	Gloss Alignment :	             D            D      D       D       S          S                
2025-06-05 10:23:58,454 	--------------------------------------------------------------------------------------------------------------------
2025-06-05 10:23:58,455 	Text Reference  :	und nun die wettervorhersage für morgen freitag    den sechsten mai   
2025-06-05 10:23:58,455 	Text Hypothesis :	und nun die wettervorhersage für morgen donnerstag den zwölften august
2025-06-05 10:23:58,455 	Text Alignment  :	                                        S              S        S     
2025-06-05 10:23:58,455 ========================================================================================================================
2025-06-05 10:23:58,455 Epoch 100: Total Training Recognition Loss 0.74  Total Training Translation Loss 0.06 
2025-06-05 10:23:58,455 EPOCH 101
2025-06-05 10:23:58,492 Epoch 101: Total Training Recognition Loss 0.77  Total Training Translation Loss 0.17 
2025-06-05 10:23:58,492 EPOCH 102
2025-06-05 10:23:58,529 Epoch 102: Total Training Recognition Loss 0.75  Total Training Translation Loss 0.06 
2025-06-05 10:23:58,529 EPOCH 103
2025-06-05 10:23:58,577 Epoch 103: Total Training Recognition Loss 0.68  Total Training Translation Loss 0.13 
2025-06-05 10:23:58,577 EPOCH 104
2025-06-05 10:23:58,613 Epoch 104: Total Training Recognition Loss 0.68  Total Training Translation Loss 0.07 
2025-06-05 10:23:58,613 EPOCH 105
2025-06-05 10:23:58,686 Epoch 105: Total Training Recognition Loss 0.68  Total Training Translation Loss 0.12 
2025-06-05 10:23:58,687 EPOCH 106
2025-06-05 10:23:58,745 Epoch 106: Total Training Recognition Loss 0.69  Total Training Translation Loss 0.05 
2025-06-05 10:23:58,745 EPOCH 107
2025-06-05 10:23:58,780 Epoch 107: Total Training Recognition Loss 0.64  Total Training Translation Loss 0.08 
2025-06-05 10:23:58,781 EPOCH 108
2025-06-05 10:23:58,816 Epoch 108: Total Training Recognition Loss 0.64  Total Training Translation Loss 0.11 
2025-06-05 10:23:58,816 EPOCH 109
2025-06-05 10:23:58,852 Epoch 109: Total Training Recognition Loss 0.60  Total Training Translation Loss 0.10 
2025-06-05 10:23:58,852 EPOCH 110
2025-06-05 10:23:58,889 Epoch 110: Total Training Recognition Loss 0.60  Total Training Translation Loss 0.04 
2025-06-05 10:23:58,889 EPOCH 111
2025-06-05 10:23:58,929 Epoch 111: Total Training Recognition Loss 0.59  Total Training Translation Loss 0.18 
2025-06-05 10:23:58,929 EPOCH 112
2025-06-05 10:23:58,965 Epoch 112: Total Training Recognition Loss 0.55  Total Training Translation Loss 0.22 
2025-06-05 10:23:58,966 EPOCH 113
2025-06-05 10:23:59,004 Epoch 113: Total Training Recognition Loss 0.54  Total Training Translation Loss 0.07 
2025-06-05 10:23:59,005 EPOCH 114
2025-06-05 10:23:59,044 Epoch 114: Total Training Recognition Loss 0.58  Total Training Translation Loss 0.04 
2025-06-05 10:23:59,044 EPOCH 115
2025-06-05 10:23:59,082 Epoch 115: Total Training Recognition Loss 0.55  Total Training Translation Loss 0.30 
2025-06-05 10:23:59,083 EPOCH 116
2025-06-05 10:23:59,120 Epoch 116: Total Training Recognition Loss 0.49  Total Training Translation Loss 0.05 
2025-06-05 10:23:59,121 EPOCH 117
2025-06-05 10:23:59,157 Epoch 117: Total Training Recognition Loss 0.47  Total Training Translation Loss 0.02 
2025-06-05 10:23:59,157 EPOCH 118
2025-06-05 10:23:59,197 Epoch 118: Total Training Recognition Loss 0.45  Total Training Translation Loss 0.03 
2025-06-05 10:23:59,197 EPOCH 119
2025-06-05 10:23:59,235 Epoch 119: Total Training Recognition Loss 0.46  Total Training Translation Loss 0.09 
2025-06-05 10:23:59,235 EPOCH 120
2025-06-05 10:23:59,271 Epoch 120: Total Training Recognition Loss 0.41  Total Training Translation Loss 0.22 
2025-06-05 10:23:59,271 EPOCH 121
2025-06-05 10:23:59,309 Epoch 121: Total Training Recognition Loss 0.42  Total Training Translation Loss 0.08 
2025-06-05 10:23:59,309 EPOCH 122
2025-06-05 10:23:59,348 Epoch 122: Total Training Recognition Loss 0.40  Total Training Translation Loss 0.05 
2025-06-05 10:23:59,348 EPOCH 123
2025-06-05 10:23:59,386 Epoch 123: Total Training Recognition Loss 0.39  Total Training Translation Loss 0.16 
2025-06-05 10:23:59,387 EPOCH 124
2025-06-05 10:23:59,461 Epoch 124: Total Training Recognition Loss 0.39  Total Training Translation Loss 0.25 
2025-06-05 10:23:59,461 EPOCH 125
2025-06-05 10:23:59,507 Epoch 125: Total Training Recognition Loss 0.38  Total Training Translation Loss 0.05 
2025-06-05 10:23:59,507 EPOCH 126
2025-06-05 10:23:59,544 Epoch 126: Total Training Recognition Loss 0.34  Total Training Translation Loss 0.05 
2025-06-05 10:23:59,545 EPOCH 127
2025-06-05 10:23:59,581 Epoch 127: Total Training Recognition Loss 0.28  Total Training Translation Loss 0.04 
2025-06-05 10:23:59,581 EPOCH 128
2025-06-05 10:23:59,619 Epoch 128: Total Training Recognition Loss 0.30  Total Training Translation Loss 0.10 
2025-06-05 10:23:59,619 EPOCH 129
2025-06-05 10:23:59,657 Epoch 129: Total Training Recognition Loss 0.29  Total Training Translation Loss 0.11 
2025-06-05 10:23:59,657 EPOCH 130
2025-06-05 10:23:59,694 Epoch 130: Total Training Recognition Loss 0.29  Total Training Translation Loss 0.13 
2025-06-05 10:23:59,694 EPOCH 131
2025-06-05 10:23:59,733 Epoch 131: Total Training Recognition Loss 0.30  Total Training Translation Loss 0.23 
2025-06-05 10:23:59,733 EPOCH 132
2025-06-05 10:23:59,770 Epoch 132: Total Training Recognition Loss 0.29  Total Training Translation Loss 0.06 
2025-06-05 10:23:59,770 EPOCH 133
2025-06-05 10:23:59,808 Epoch 133: Total Training Recognition Loss 0.25  Total Training Translation Loss 0.17 
2025-06-05 10:23:59,809 EPOCH 134
2025-06-05 10:23:59,846 Epoch 134: Total Training Recognition Loss 0.27  Total Training Translation Loss 0.10 
2025-06-05 10:23:59,846 EPOCH 135
2025-06-05 10:23:59,883 Epoch 135: Total Training Recognition Loss 0.24  Total Training Translation Loss 0.04 
2025-06-05 10:23:59,883 EPOCH 136
2025-06-05 10:23:59,919 Epoch 136: Total Training Recognition Loss 0.22  Total Training Translation Loss 0.07 
2025-06-05 10:23:59,919 EPOCH 137
2025-06-05 10:23:59,955 Epoch 137: Total Training Recognition Loss 0.20  Total Training Translation Loss 0.49 
2025-06-05 10:23:59,955 EPOCH 138
2025-06-05 10:23:59,990 Epoch 138: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.05 
2025-06-05 10:23:59,990 EPOCH 139
2025-06-05 10:24:00,026 Epoch 139: Total Training Recognition Loss 0.21  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,026 EPOCH 140
2025-06-05 10:24:00,062 Epoch 140: Total Training Recognition Loss 0.18  Total Training Translation Loss 0.17 
2025-06-05 10:24:00,062 EPOCH 141
2025-06-05 10:24:00,099 Epoch 141: Total Training Recognition Loss 0.18  Total Training Translation Loss 0.39 
2025-06-05 10:24:00,099 EPOCH 142
2025-06-05 10:24:00,135 Epoch 142: Total Training Recognition Loss 0.15  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,135 EPOCH 143
2025-06-05 10:24:00,171 Epoch 143: Total Training Recognition Loss 0.16  Total Training Translation Loss 0.02 
2025-06-05 10:24:00,172 EPOCH 144
2025-06-05 10:24:00,207 Epoch 144: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.15 
2025-06-05 10:24:00,207 EPOCH 145
2025-06-05 10:24:00,243 Epoch 145: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.02 
2025-06-05 10:24:00,243 EPOCH 146
2025-06-05 10:24:00,279 Epoch 146: Total Training Recognition Loss 0.17  Total Training Translation Loss 0.24 
2025-06-05 10:24:00,279 EPOCH 147
2025-06-05 10:24:00,315 Epoch 147: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.63 
2025-06-05 10:24:00,316 EPOCH 148
2025-06-05 10:24:00,352 Epoch 148: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.05 
2025-06-05 10:24:00,352 EPOCH 149
2025-06-05 10:24:00,388 Epoch 149: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.42 
2025-06-05 10:24:00,388 EPOCH 150
2025-06-05 10:24:00,424 Epoch 150: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.12 
2025-06-05 10:24:00,424 EPOCH 151
2025-06-05 10:24:00,461 Epoch 151: Total Training Recognition Loss 0.13  Total Training Translation Loss 0.08 
2025-06-05 10:24:00,461 EPOCH 152
2025-06-05 10:24:00,497 Epoch 152: Total Training Recognition Loss 0.11  Total Training Translation Loss 0.03 
2025-06-05 10:24:00,497 EPOCH 153
2025-06-05 10:24:00,534 Epoch 153: Total Training Recognition Loss 0.14  Total Training Translation Loss 0.03 
2025-06-05 10:24:00,534 EPOCH 154
2025-06-05 10:24:00,571 Epoch 154: Total Training Recognition Loss 0.12  Total Training Translation Loss 0.24 
2025-06-05 10:24:00,571 EPOCH 155
2025-06-05 10:24:00,607 Epoch 155: Total Training Recognition Loss 0.15  Total Training Translation Loss 0.10 
2025-06-05 10:24:00,607 EPOCH 156
2025-06-05 10:24:00,643 Epoch 156: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.22 
2025-06-05 10:24:00,643 EPOCH 157
2025-06-05 10:24:00,679 Epoch 157: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.08 
2025-06-05 10:24:00,679 EPOCH 158
2025-06-05 10:24:00,714 Epoch 158: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,715 EPOCH 159
2025-06-05 10:24:00,751 Epoch 159: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.27 
2025-06-05 10:24:00,751 EPOCH 160
2025-06-05 10:24:00,787 Epoch 160: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,787 EPOCH 161
2025-06-05 10:24:00,822 Epoch 161: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.22 
2025-06-05 10:24:00,823 EPOCH 162
2025-06-05 10:24:00,859 Epoch 162: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,859 EPOCH 163
2025-06-05 10:24:00,894 Epoch 163: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.19 
2025-06-05 10:24:00,894 EPOCH 164
2025-06-05 10:24:00,934 Epoch 164: Total Training Recognition Loss 0.08  Total Training Translation Loss 0.11 
2025-06-05 10:24:00,934 EPOCH 165
2025-06-05 10:24:00,973 Epoch 165: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.04 
2025-06-05 10:24:00,973 EPOCH 166
2025-06-05 10:24:01,009 Epoch 166: Total Training Recognition Loss 0.09  Total Training Translation Loss 0.13 
2025-06-05 10:24:01,009 EPOCH 167
2025-06-05 10:24:01,044 Epoch 167: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.27 
2025-06-05 10:24:01,044 EPOCH 168
2025-06-05 10:24:01,082 Epoch 168: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.03 
2025-06-05 10:24:01,082 EPOCH 169
2025-06-05 10:24:01,119 Epoch 169: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.07 
2025-06-05 10:24:01,120 EPOCH 170
2025-06-05 10:24:01,158 Epoch 170: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.15 
2025-06-05 10:24:01,158 EPOCH 171
2025-06-05 10:24:01,194 Epoch 171: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.04 
2025-06-05 10:24:01,195 EPOCH 172
2025-06-05 10:24:01,230 Epoch 172: Total Training Recognition Loss 0.07  Total Training Translation Loss 0.07 
2025-06-05 10:24:01,230 EPOCH 173
2025-06-05 10:24:01,266 Epoch 173: Total Training Recognition Loss 0.06  Total Training Translation Loss 0.38 
2025-06-05 10:24:01,266 EPOCH 174
2025-06-05 10:24:01,302 Epoch 174: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.06 
2025-06-05 10:24:01,302 EPOCH 175
2025-06-05 10:24:01,340 Epoch 175: Total Training Recognition Loss 0.10  Total Training Translation Loss 0.04 
2025-06-05 10:24:01,340 EPOCH 176
2025-06-05 10:24:01,377 Epoch 176: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.08 
2025-06-05 10:24:01,377 EPOCH 177
2025-06-05 10:24:01,413 Epoch 177: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.74 
2025-06-05 10:24:01,414 EPOCH 178
2025-06-05 10:24:01,454 Epoch 178: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.11 
2025-06-05 10:24:01,454 EPOCH 179
2025-06-05 10:24:01,492 Epoch 179: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.05 
2025-06-05 10:24:01,492 EPOCH 180
2025-06-05 10:24:01,531 Epoch 180: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.08 
2025-06-05 10:24:01,531 EPOCH 181
2025-06-05 10:24:01,568 Epoch 181: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.03 
2025-06-05 10:24:01,569 EPOCH 182
2025-06-05 10:24:01,605 Epoch 182: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.48 
2025-06-05 10:24:01,605 EPOCH 183
2025-06-05 10:24:01,644 Epoch 183: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.31 
2025-06-05 10:24:01,644 EPOCH 184
2025-06-05 10:24:01,682 Epoch 184: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.07 
2025-06-05 10:24:01,682 EPOCH 185
2025-06-05 10:24:01,720 Epoch 185: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.19 
2025-06-05 10:24:01,720 EPOCH 186
2025-06-05 10:24:01,779 Epoch 186: Total Training Recognition Loss 0.05  Total Training Translation Loss 0.02 
2025-06-05 10:24:01,779 EPOCH 187
2025-06-05 10:24:01,816 Epoch 187: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.03 
2025-06-05 10:24:01,816 EPOCH 188
2025-06-05 10:24:01,856 Epoch 188: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.02 
2025-06-05 10:24:01,856 EPOCH 189
2025-06-05 10:24:01,891 Epoch 189: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.44 
2025-06-05 10:24:01,891 EPOCH 190
2025-06-05 10:24:01,933 Epoch 190: Total Training Recognition Loss 0.04  Total Training Translation Loss 0.08 
2025-06-05 10:24:01,934 EPOCH 191
2025-06-05 10:24:01,996 Epoch 191: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.04 
2025-06-05 10:24:01,996 EPOCH 192
2025-06-05 10:24:02,081 Epoch 192: Total Training Recognition Loss 0.03  Total Training Translation Loss 0.06 
2025-06-05 10:24:02,081 EPOCH 193
2025-06-05 10:24:02,129 Epoch 193: Total Training Recognition Loss 0.02  Total Training Translation Loss 0.06 
2025-06-05 10:24:02,129 EPOCH 194
